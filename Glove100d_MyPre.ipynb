{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/f0/1d9bfcc8ee6b83472ec571406bd0dd51c0e6330ff1a51b2d29861d389e85/textblob-0.15.3-py2.py3-none-any.whl (636kB)\n",
      "\u001b[K    100% |████████████████████████████████| 645kB 29.8MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: nltk>=3.1 in /data/anaconda/envs/py35/lib/python3.5/site-packages (from textblob) (3.3)\n",
      "Requirement already satisfied: six in /data/anaconda/envs/py35/lib/python3.5/site-packages (from nltk>=3.1->textblob) (1.11.0)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.15.3\n",
      "\u001b[33mYou are using pip version 19.0.2, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.utils import np_utils\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from keras.models import Model,load_model\n",
    "import textblob\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./dataset/hm_train.csv')\n",
    "df_test = pd.read_csv('./dataset/hm_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hmid</th>\n",
       "      <th>reflection_period</th>\n",
       "      <th>cleaned_hm</th>\n",
       "      <th>num_sentence</th>\n",
       "      <th>predicted_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27673</td>\n",
       "      <td>24h</td>\n",
       "      <td>I went on a successful date with someone I fel...</td>\n",
       "      <td>1</td>\n",
       "      <td>affection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27674</td>\n",
       "      <td>24h</td>\n",
       "      <td>I was happy when my son got 90% marks in his e...</td>\n",
       "      <td>1</td>\n",
       "      <td>affection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27675</td>\n",
       "      <td>24h</td>\n",
       "      <td>I went to the gym this morning and did yoga.</td>\n",
       "      <td>1</td>\n",
       "      <td>exercise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27676</td>\n",
       "      <td>24h</td>\n",
       "      <td>We had a serious talk with some friends of our...</td>\n",
       "      <td>2</td>\n",
       "      <td>bonding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27677</td>\n",
       "      <td>24h</td>\n",
       "      <td>I went with grandchildren to butterfly display...</td>\n",
       "      <td>1</td>\n",
       "      <td>affection</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hmid reflection_period                                         cleaned_hm  \\\n",
       "0  27673               24h  I went on a successful date with someone I fel...   \n",
       "1  27674               24h  I was happy when my son got 90% marks in his e...   \n",
       "2  27675               24h       I went to the gym this morning and did yoga.   \n",
       "3  27676               24h  We had a serious talk with some friends of our...   \n",
       "4  27677               24h  I went with grandchildren to butterfly display...   \n",
       "\n",
       "   num_sentence predicted_category  \n",
       "0             1          affection  \n",
       "1             1          affection  \n",
       "2             1           exercise  \n",
       "3             2            bonding  \n",
       "4             1          affection  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hmid</th>\n",
       "      <th>reflection_period</th>\n",
       "      <th>cleaned_hm</th>\n",
       "      <th>num_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88305</td>\n",
       "      <td>3m</td>\n",
       "      <td>I spent the weekend in Chicago with my friends.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88306</td>\n",
       "      <td>3m</td>\n",
       "      <td>We moved back into our house after a remodel. ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88307</td>\n",
       "      <td>3m</td>\n",
       "      <td>My fiance proposed to me in front of my family...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88308</td>\n",
       "      <td>3m</td>\n",
       "      <td>I ate lobster at a fancy restaurant with some ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88309</td>\n",
       "      <td>3m</td>\n",
       "      <td>I went out to a nice restaurant on a date with...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hmid reflection_period                                         cleaned_hm  \\\n",
       "0  88305                3m    I spent the weekend in Chicago with my friends.   \n",
       "1  88306                3m  We moved back into our house after a remodel. ...   \n",
       "2  88307                3m  My fiance proposed to me in front of my family...   \n",
       "3  88308                3m  I ate lobster at a fancy restaurant with some ...   \n",
       "4  88309                3m  I went out to a nice restaurant on a date with...   \n",
       "\n",
       "   num_sentence  \n",
       "0             1  \n",
       "1             2  \n",
       "2             1  \n",
       "3             1  \n",
       "4             5  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60321, 5)\n",
      "(40213, 4)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array(['achievement', 'affection', 'bonding', 'enjoy_the_moment',\n",
      "       'exercise', 'leisure', 'nature'], dtype=object), array([20274, 20880,  6561,  6508,   729,  4242,  1127]))\n"
     ]
    }
   ],
   "source": [
    "labels = df_train['predicted_category']\n",
    "print(np.unique(labels,return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = { 0 : 'achievement', 1 : 'affection', 2 : 'bonding', 3 : 'enjoy_the_moment', 4 : 'exercise', 5 : 'leisure', 6 : 'nature'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_mapping = { 'achievement' : 0, 'affection' : 1, 'bonding' : 2, 'enjoy_the_moment' : 3, 'exercise' : 4, 'leisure' : 5, 'nature' : 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['predicted_category'].replace(rev_mapping, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hmid</th>\n",
       "      <th>reflection_period</th>\n",
       "      <th>cleaned_hm</th>\n",
       "      <th>num_sentence</th>\n",
       "      <th>predicted_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27673</td>\n",
       "      <td>24h</td>\n",
       "      <td>I went on a successful date with someone I fel...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27674</td>\n",
       "      <td>24h</td>\n",
       "      <td>I was happy when my son got 90% marks in his e...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27675</td>\n",
       "      <td>24h</td>\n",
       "      <td>I went to the gym this morning and did yoga.</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27676</td>\n",
       "      <td>24h</td>\n",
       "      <td>We had a serious talk with some friends of our...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27677</td>\n",
       "      <td>24h</td>\n",
       "      <td>I went with grandchildren to butterfly display...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hmid reflection_period                                         cleaned_hm  \\\n",
       "0  27673               24h  I went on a successful date with someone I fel...   \n",
       "1  27674               24h  I was happy when my son got 90% marks in his e...   \n",
       "2  27675               24h       I went to the gym this morning and did yoga.   \n",
       "3  27676               24h  We had a serious talk with some friends of our...   \n",
       "4  27677               24h  I went with grandchildren to butterfly display...   \n",
       "\n",
       "   num_sentence  predicted_category  \n",
       "0             1                   1  \n",
       "1             1                   1  \n",
       "2             1                   4  \n",
       "3             2                   2  \n",
       "4             1                   1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame()\n",
    "test = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60321, 3) (40213, 2)\n"
     ]
    }
   ],
   "source": [
    "train['Phrase'] = df_train['cleaned_hm']\n",
    "train['Sentiment'] = df_train['predicted_category']\n",
    "train['SentenceId'] = df_train['hmid']\n",
    "\n",
    "test['Phrase'] = df_test['cleaned_hm']\n",
    "test['SentenceId'] = df_test['hmid']\n",
    "print (train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip(' ')\n",
    "    return text\n",
    "\n",
    "train['Phrase'] = train['Phrase'].map(lambda com : clean_text(com))\n",
    "test['Phrase'] = test['Phrase'].map(lambda com : clean_text(com))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Vocabulary Size: 20509\n",
      "Test Set Vocabulary Size: 16946\n",
      "Number of Words that occur in both: 12141\n"
     ]
    }
   ],
   "source": [
    "cv1 = CountVectorizer()\n",
    "cv1.fit(train[\"Phrase\"])\n",
    "\n",
    "cv2 = CountVectorizer()\n",
    "cv2.fit(test[\"Phrase\"])\n",
    "\n",
    "print(\"Train Set Vocabulary Size:\", len(cv1.vocabulary_))\n",
    "print(\"Test Set Vocabulary Size:\", len(cv2.vocabulary_))\n",
    "print(\"Number of Words that occur in both:\", len(set(cv1.vocabulary_.keys()).intersection(set(cv2.vocabulary_.keys()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_count</th>\n",
       "      <th>has_upper</th>\n",
       "      <th>after_comma</th>\n",
       "      <th>sentence_end</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.812913</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.190756</td>\n",
       "      <td>0.414953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.344157</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.239366</td>\n",
       "      <td>0.429889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.489864</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.257471</td>\n",
       "      <td>0.394704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21.589121</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.334586</td>\n",
       "      <td>0.544569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.489712</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.241670</td>\n",
       "      <td>0.380579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.437294</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.156239</td>\n",
       "      <td>0.382447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>18.655723</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.295409</td>\n",
       "      <td>0.539620</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           word_count  has_upper  after_comma  sentence_end  polarity  \\\n",
       "Sentiment                                                               \n",
       "0           16.812913      False        False         False  0.190756   \n",
       "1           21.344157      False        False         False  0.239366   \n",
       "2           19.489864      False        False         False  0.257471   \n",
       "3           21.589121      False        False         False  0.334586   \n",
       "4           13.489712      False        False         False  0.241670   \n",
       "5           12.437294      False        False         False  0.156239   \n",
       "6           18.655723      False        False         False  0.295409   \n",
       "\n",
       "           subjectivity  \n",
       "Sentiment                \n",
       "0              0.414953  \n",
       "1              0.429889  \n",
       "2              0.394704  \n",
       "3              0.544569  \n",
       "4              0.380579  \n",
       "5              0.382447  \n",
       "6              0.539620  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform(df):\n",
    "    df[\"word_count\"] = df[\"Phrase\"].apply(lambda x: len(x.split()))\n",
    "    df[\"has_upper\"] = df[\"Phrase\"].apply(lambda x: x.lower() != x)\n",
    "    df[\"sentence_end\"] = df[\"Phrase\"].apply(lambda x: x.endswith(\".\"))\n",
    "    df[\"after_comma\"] = df[\"Phrase\"].apply(lambda x: x.startswith(\",\"))\n",
    "    df[\"Phrase\"] = df[\"Phrase\"].apply(lambda x: x.lower())\n",
    "    return df\n",
    "\n",
    "train = transform(train)\n",
    "test = transform(test)\n",
    "\n",
    "def getSentFeat(s , polarity):\n",
    "    sent = textblob.TextBlob(s).sentiment\n",
    "    if polarity:\n",
    "        return sent.polarity\n",
    "    else :\n",
    "        return sent.subjectivity\n",
    "    \n",
    "train['polarity'] = train['Phrase'].apply(lambda x: getSentFeat(x , polarity=True))\n",
    "train['subjectivity'] = train['Phrase'].apply(lambda x: getSentFeat(x , polarity=False))\n",
    "\n",
    "test['polarity'] = test['Phrase'].apply(lambda x: getSentFeat(x , polarity=True))\n",
    "test['subjectivity'] = test['Phrase'].apply(lambda x: getSentFeat(x , polarity=False))\n",
    "\n",
    "dense_features = [\"word_count\", \"has_upper\", \"after_comma\", \"sentence_end\" ,\"polarity\",\"subjectivity\"]\n",
    "\n",
    "train.groupby(\"Sentiment\")[dense_features].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FOLDS = 5\n",
    "\n",
    "train[\"fold_id\"] = train[\"SentenceId\"].apply(lambda x: x%NUM_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words that don't exist in GLOVE: 2219\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_FILE = \"./glove.6B.100d.txt\"\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "all_words = set(cv1.vocabulary_.keys()).union(set(cv2.vocabulary_.keys()))\n",
    "\n",
    "def get_embedding():\n",
    "    embeddings_index = {}\n",
    "    f = open(EMBEDDING_FILE)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        if len(values) == EMBEDDING_DIM + 1 and word in all_words:\n",
    "            coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "            embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeddings_index\n",
    "\n",
    "embeddings_index = get_embedding()\n",
    "print(\"Number of words that don't exist in GLOVE:\", len(all_words - set(embeddings_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60321, 60)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 60\n",
    "\n",
    "tokenizer = Tokenizer(filters=\"\")\n",
    "tokenizer.fit_on_texts(np.append(train[\"Phrase\"].values, test[\"Phrase\"].values))\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "nb_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "seq = pad_sequences(tokenizer.texts_to_sequences(train[\"Phrase\"]), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_seq = pad_sequences(tokenizer.texts_to_sequences(test[\"Phrase\"]), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True)\n",
    "    dropout = SpatialDropout1D(0.2)\n",
    "    mask_layer = Masking()\n",
    "    lstm_layer = LSTM(50)\n",
    "    \n",
    "    seq_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\")\n",
    "    dense_input = Input(shape=(len(dense_features),))\n",
    "    \n",
    "    dense_vector = BatchNormalization()(dense_input)\n",
    "    \n",
    "    phrase_vector = lstm_layer(mask_layer(dropout(embedding_layer(seq_input))))\n",
    "    \n",
    "    feature_vector = concatenate([phrase_vector, dense_vector])\n",
    "    feature_vector = Dense(50, activation=\"relu\")(feature_vector)\n",
    "    feature_vector = Dense(20, activation=\"relu\")(feature_vector)\n",
    "    \n",
    "    output = Dense(7, activation=\"softmax\")(feature_vector)\n",
    "    \n",
    "    model = Model(inputs=[seq_input, dense_input], outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = OneHotEncoder(sparse=False)\n",
    "enc.fit(train[\"Sentiment\"].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 1\n",
      "Splitting the data into train and validation...\n",
      "Building the model...\n",
      "Training the model...\n",
      "Train on 48253 samples, validate on 12068 samples\n",
      "Epoch 1/15\n",
      "48253/48253 [==============================] - 9s 196us/step - loss: 1.2570 - acc: 0.5471 - val_loss: 0.8763 - val_acc: 0.6974\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.87628, saving model to best_model.hdf5\n",
      "Epoch 2/15\n",
      "48253/48253 [==============================] - 8s 165us/step - loss: 0.6255 - acc: 0.7861 - val_loss: 0.4524 - val_acc: 0.8382\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.87628 to 0.45242, saving model to best_model.hdf5\n",
      "Epoch 3/15\n",
      "48253/48253 [==============================] - 8s 164us/step - loss: 0.4222 - acc: 0.8529 - val_loss: 0.3745 - val_acc: 0.8632\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.45242 to 0.37450, saving model to best_model.hdf5\n",
      "Epoch 4/15\n",
      "48253/48253 [==============================] - 8s 162us/step - loss: 0.3348 - acc: 0.8819 - val_loss: 0.3334 - val_acc: 0.8784\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.37450 to 0.33338, saving model to best_model.hdf5\n",
      "Epoch 5/15\n",
      "48253/48253 [==============================] - 8s 162us/step - loss: 0.2781 - acc: 0.9021 - val_loss: 0.3009 - val_acc: 0.8934\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.33338 to 0.30094, saving model to best_model.hdf5\n",
      "Epoch 6/15\n",
      "48253/48253 [==============================] - 8s 164us/step - loss: 0.2322 - acc: 0.9163 - val_loss: 0.3085 - val_acc: 0.8933\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.30094\n",
      "Epoch 7/15\n",
      "48253/48253 [==============================] - 8s 165us/step - loss: 0.1994 - acc: 0.9281 - val_loss: 0.3142 - val_acc: 0.8944\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.30094\n",
      "Epoch 8/15\n",
      "48253/48253 [==============================] - 8s 163us/step - loss: 0.1691 - acc: 0.9379 - val_loss: 0.3532 - val_acc: 0.8764\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.30094\n",
      "Epoch 9/15\n",
      "48253/48253 [==============================] - 8s 161us/step - loss: 0.1509 - acc: 0.9463 - val_loss: 0.2713 - val_acc: 0.9104\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.30094 to 0.27127, saving model to best_model.hdf5\n",
      "Epoch 10/15\n",
      "48253/48253 [==============================] - 8s 162us/step - loss: 0.1351 - acc: 0.9506 - val_loss: 0.2837 - val_acc: 0.9050\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.27127\n",
      "Epoch 11/15\n",
      "48253/48253 [==============================] - 8s 166us/step - loss: 0.1135 - acc: 0.9585 - val_loss: 0.2886 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.27127\n",
      "Epoch 00011: early stopping\n",
      "Predicting...\n",
      "40213/40213 [==============================] - 2s 42us/step\n",
      "\n",
      "FOLD 2\n",
      "Splitting the data into train and validation...\n",
      "Building the model...\n",
      "Training the model...\n",
      "Train on 48260 samples, validate on 12061 samples\n",
      "Epoch 1/15\n",
      "48260/48260 [==============================] - 9s 193us/step - loss: 1.3423 - acc: 0.5153 - val_loss: 0.8902 - val_acc: 0.6891\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.89023, saving model to best_model.hdf5\n",
      "Epoch 2/15\n",
      "48260/48260 [==============================] - 8s 161us/step - loss: 0.6633 - acc: 0.7780 - val_loss: 0.4760 - val_acc: 0.8335\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.89023 to 0.47602, saving model to best_model.hdf5\n",
      "Epoch 3/15\n",
      "48260/48260 [==============================] - 8s 163us/step - loss: 0.4472 - acc: 0.8448 - val_loss: 0.3948 - val_acc: 0.8635\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.47602 to 0.39484, saving model to best_model.hdf5\n",
      "Epoch 4/15\n",
      "48260/48260 [==============================] - 8s 162us/step - loss: 0.3505 - acc: 0.8778 - val_loss: 0.3490 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.39484 to 0.34904, saving model to best_model.hdf5\n",
      "Epoch 5/15\n",
      "48260/48260 [==============================] - 8s 166us/step - loss: 0.2848 - acc: 0.8982 - val_loss: 0.3289 - val_acc: 0.8819\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.34904 to 0.32886, saving model to best_model.hdf5\n",
      "Epoch 6/15\n",
      "48260/48260 [==============================] - 8s 163us/step - loss: 0.2359 - acc: 0.9153 - val_loss: 0.2901 - val_acc: 0.8991\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.32886 to 0.29007, saving model to best_model.hdf5\n",
      "Epoch 7/15\n",
      "48260/48260 [==============================] - 8s 162us/step - loss: 0.2071 - acc: 0.9239 - val_loss: 0.3260 - val_acc: 0.8888\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.29007\n",
      "Epoch 8/15\n",
      "48260/48260 [==============================] - 8s 162us/step - loss: 0.1774 - acc: 0.9369 - val_loss: 0.2966 - val_acc: 0.8968\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.29007\n",
      "Epoch 00008: early stopping\n",
      "Predicting...\n",
      "40213/40213 [==============================] - 2s 49us/step\n",
      "\n",
      "FOLD 3\n",
      "Splitting the data into train and validation...\n",
      "Building the model...\n",
      "Training the model...\n",
      "Train on 48247 samples, validate on 12074 samples\n",
      "Epoch 1/15\n",
      "48247/48247 [==============================] - 9s 189us/step - loss: 1.3440 - acc: 0.5145 - val_loss: 0.9889 - val_acc: 0.6690\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.98890, saving model to best_model.hdf5\n",
      "Epoch 2/15\n",
      "48247/48247 [==============================] - 8s 165us/step - loss: 0.7008 - acc: 0.7612 - val_loss: 0.5586 - val_acc: 0.8103\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.98890 to 0.55863, saving model to best_model.hdf5\n",
      "Epoch 3/15\n",
      "48247/48247 [==============================] - 8s 164us/step - loss: 0.4489 - acc: 0.8461 - val_loss: 0.4454 - val_acc: 0.8384\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.55863 to 0.44536, saving model to best_model.hdf5\n",
      "Epoch 4/15\n",
      "48247/48247 [==============================] - 8s 161us/step - loss: 0.3528 - acc: 0.8770 - val_loss: 0.3669 - val_acc: 0.8719\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.44536 to 0.36686, saving model to best_model.hdf5\n",
      "Epoch 5/15\n",
      "48247/48247 [==============================] - 8s 162us/step - loss: 0.2897 - acc: 0.8980 - val_loss: 0.3142 - val_acc: 0.8904\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.36686 to 0.31420, saving model to best_model.hdf5\n",
      "Epoch 6/15\n",
      "48247/48247 [==============================] - 8s 168us/step - loss: 0.2453 - acc: 0.9126 - val_loss: 0.2834 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.31420 to 0.28340, saving model to best_model.hdf5\n",
      "Epoch 7/15\n",
      "48247/48247 [==============================] - 8s 162us/step - loss: 0.2083 - acc: 0.9247 - val_loss: 0.2949 - val_acc: 0.8962\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.28340\n",
      "Epoch 8/15\n",
      "48247/48247 [==============================] - 8s 161us/step - loss: 0.1792 - acc: 0.9361 - val_loss: 0.3298 - val_acc: 0.8917\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.28340\n",
      "Epoch 00008: early stopping\n",
      "Predicting...\n",
      "40213/40213 [==============================] - 2s 45us/step\n",
      "\n",
      "FOLD 4\n",
      "Splitting the data into train and validation...\n",
      "Building the model...\n",
      "Training the model...\n",
      "Train on 48255 samples, validate on 12066 samples\n",
      "Epoch 1/15\n",
      "48255/48255 [==============================] - 9s 196us/step - loss: 1.3335 - acc: 0.5352 - val_loss: 0.8764 - val_acc: 0.7072\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.87643, saving model to best_model.hdf5\n",
      "Epoch 2/15\n",
      "48255/48255 [==============================] - 8s 161us/step - loss: 0.6351 - acc: 0.7898 - val_loss: 0.4693 - val_acc: 0.8392\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.87643 to 0.46926, saving model to best_model.hdf5\n",
      "Epoch 3/15\n",
      "48255/48255 [==============================] - 8s 167us/step - loss: 0.4274 - acc: 0.8520 - val_loss: 0.3975 - val_acc: 0.8597\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.46926 to 0.39753, saving model to best_model.hdf5\n",
      "Epoch 4/15\n",
      "48255/48255 [==============================] - 8s 162us/step - loss: 0.3455 - acc: 0.8777 - val_loss: 0.3428 - val_acc: 0.8774\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.39753 to 0.34278, saving model to best_model.hdf5\n",
      "Epoch 5/15\n",
      "48255/48255 [==============================] - 8s 163us/step - loss: 0.2795 - acc: 0.9009 - val_loss: 0.3682 - val_acc: 0.8717\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.34278\n",
      "Epoch 6/15\n",
      "48255/48255 [==============================] - 8s 162us/step - loss: 0.2308 - acc: 0.9170 - val_loss: 0.3147 - val_acc: 0.8940\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.34278 to 0.31465, saving model to best_model.hdf5\n",
      "Epoch 7/15\n",
      "48255/48255 [==============================] - 8s 167us/step - loss: 0.1986 - acc: 0.9282 - val_loss: 0.2971 - val_acc: 0.8956\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.31465 to 0.29712, saving model to best_model.hdf5\n",
      "Epoch 8/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48255/48255 [==============================] - 8s 163us/step - loss: 0.1709 - acc: 0.9381 - val_loss: 0.2950 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.29712 to 0.29501, saving model to best_model.hdf5\n",
      "Epoch 9/15\n",
      "48255/48255 [==============================] - 8s 160us/step - loss: 0.1465 - acc: 0.9468 - val_loss: 0.3288 - val_acc: 0.8971\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.29501\n",
      "Epoch 10/15\n",
      "48255/48255 [==============================] - 8s 162us/step - loss: 0.1300 - acc: 0.9524 - val_loss: 0.2882 - val_acc: 0.9087\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.29501 to 0.28824, saving model to best_model.hdf5\n",
      "Epoch 11/15\n",
      "48255/48255 [==============================] - 8s 164us/step - loss: 0.1117 - acc: 0.9591 - val_loss: 0.3202 - val_acc: 0.9020\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.28824\n",
      "Epoch 12/15\n",
      "48255/48255 [==============================] - 8s 165us/step - loss: 0.1010 - acc: 0.9623 - val_loss: 0.2912 - val_acc: 0.9128\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.28824\n",
      "Epoch 13/15\n",
      "48255/48255 [==============================] - 8s 162us/step - loss: 0.0881 - acc: 0.9677 - val_loss: 0.3084 - val_acc: 0.9107\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.28824\n",
      "Epoch 14/15\n",
      "48255/48255 [==============================] - 8s 162us/step - loss: 0.0783 - acc: 0.9718 - val_loss: 0.3341 - val_acc: 0.9066\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.28824\n",
      "Epoch 00014: early stopping\n",
      "Predicting...\n",
      "40213/40213 [==============================] - 2s 45us/step\n",
      "\n",
      "FOLD 5\n",
      "Splitting the data into train and validation...\n",
      "Building the model...\n",
      "Training the model...\n",
      "Train on 48269 samples, validate on 12052 samples\n",
      "Epoch 1/15\n",
      "48269/48269 [==============================] - 10s 206us/step - loss: 1.2233 - acc: 0.5570 - val_loss: 0.8237 - val_acc: 0.7056\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.82368, saving model to best_model.hdf5\n",
      "Epoch 2/15\n",
      "48269/48269 [==============================] - 8s 165us/step - loss: 0.6525 - acc: 0.7740 - val_loss: 0.4680 - val_acc: 0.8377\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.82368 to 0.46802, saving model to best_model.hdf5\n",
      "Epoch 3/15\n",
      "48269/48269 [==============================] - 8s 162us/step - loss: 0.4163 - acc: 0.8539 - val_loss: 0.3848 - val_acc: 0.8628\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.46802 to 0.38480, saving model to best_model.hdf5\n",
      "Epoch 4/15\n",
      "48269/48269 [==============================] - 8s 162us/step - loss: 0.3326 - acc: 0.8811 - val_loss: 0.3409 - val_acc: 0.8813\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.38480 to 0.34090, saving model to best_model.hdf5\n",
      "Epoch 5/15\n",
      "48269/48269 [==============================] - 8s 162us/step - loss: 0.2798 - acc: 0.8997 - val_loss: 0.3355 - val_acc: 0.8852\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.34090 to 0.33548, saving model to best_model.hdf5\n",
      "Epoch 6/15\n",
      "48269/48269 [==============================] - 8s 166us/step - loss: 0.2339 - acc: 0.9140 - val_loss: 0.2985 - val_acc: 0.8945\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.33548 to 0.29852, saving model to best_model.hdf5\n",
      "Epoch 7/15\n",
      "48269/48269 [==============================] - 8s 162us/step - loss: 0.2040 - acc: 0.9262 - val_loss: 0.5331 - val_acc: 0.8335\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.29852\n",
      "Epoch 8/15\n",
      "48269/48269 [==============================] - 8s 162us/step - loss: 0.1794 - acc: 0.9353 - val_loss: 0.2782 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.29852 to 0.27819, saving model to best_model.hdf5\n",
      "Epoch 9/15\n",
      "48269/48269 [==============================] - 8s 160us/step - loss: 0.1496 - acc: 0.9450 - val_loss: 0.2822 - val_acc: 0.9047\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.27819\n",
      "Epoch 10/15\n",
      "48269/48269 [==============================] - 8s 163us/step - loss: 0.1295 - acc: 0.9521 - val_loss: 0.3117 - val_acc: 0.8943\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.27819\n",
      "Epoch 11/15\n",
      "48269/48269 [==============================] - 8s 163us/step - loss: 0.1162 - acc: 0.9574 - val_loss: 0.2937 - val_acc: 0.9085\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.27819\n",
      "Epoch 12/15\n",
      "48269/48269 [==============================] - 8s 161us/step - loss: 0.0994 - acc: 0.9640 - val_loss: 0.3607 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.27819\n",
      "Epoch 13/15\n",
      "48269/48269 [==============================] - 8s 162us/step - loss: 0.0904 - acc: 0.9666 - val_loss: 0.3038 - val_acc: 0.9101\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.27819\n",
      "Epoch 14/15\n",
      "48269/48269 [==============================] - 8s 161us/step - loss: 0.0787 - acc: 0.9706 - val_loss: 0.3226 - val_acc: 0.9091\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.27819\n",
      "Epoch 15/15\n",
      "48269/48269 [==============================] - 8s 166us/step - loss: 0.0752 - acc: 0.9721 - val_loss: 0.3331 - val_acc: 0.9037\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.27819\n",
      "Epoch 00015: early stopping\n",
      "Predicting...\n",
      "40213/40213 [==============================] - 2s 49us/step\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_preds = np.zeros((test.shape[0], 7))\n",
    "\n",
    "for i in range(NUM_FOLDS):\n",
    "    print(\"FOLD\", i+1)\n",
    "    \n",
    "    print(\"Splitting the data into train and validation...\")\n",
    "    train_seq, val_seq = seq[train[\"fold_id\"] != i], seq[train[\"fold_id\"] == i]\n",
    "    train_dense, val_dense = train[train[\"fold_id\"] != i][dense_features], train[train[\"fold_id\"] == i][dense_features]\n",
    "    y_train = enc.transform(train[train[\"fold_id\"] != i][\"Sentiment\"].values.reshape(-1, 1))\n",
    "    y_val = enc.transform(train[train[\"fold_id\"] == i][\"Sentiment\"].values.reshape(-1, 1))\n",
    "    \n",
    "    print(\"Building the model...\")\n",
    "    model = build_model()\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"acc\"])\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor=\"val_acc\", patience=2, verbose=1)\n",
    "    file_path = \"best_model.hdf5\"\n",
    "    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1, save_best_only = True, mode = \"min\")\n",
    "    \n",
    "    print(\"Training the model...\")\n",
    "    model.fit([train_seq, train_dense], y_train, validation_data=([val_seq, val_dense], y_val),\n",
    "              epochs=15, batch_size=1024, shuffle=True, callbacks=[check_point,early_stopping], verbose=1)\n",
    "    \n",
    "    print(\"Predicting...\")\n",
    "    test_preds += model.predict([test_seq, test[dense_features]], batch_size=1024, verbose=1)\n",
    "    print()\n",
    "    \n",
    "test_preds /= NUM_FOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select the class with the highest probability as prediction...\n",
      "Make the submission ready...\n"
     ]
    }
   ],
   "source": [
    "print(\"Select the class with the highest probability as prediction...\")\n",
    "test[\"pred\"] = test_preds.argmax(axis=1)\n",
    "\n",
    "\n",
    "print(\"Make the submission ready...\")\n",
    "test[\"Sentiment\"] = test[\"pred\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_final = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_final['hmid'] = test['SentenceId']\n",
    "ans_final['predicted_category'] = test['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_final['predicted_category'].replace(label_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hmid</th>\n",
       "      <th>predicted_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88305</td>\n",
       "      <td>bonding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88306</td>\n",
       "      <td>affection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88307</td>\n",
       "      <td>affection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88308</td>\n",
       "      <td>bonding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88309</td>\n",
       "      <td>affection</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hmid predicted_category\n",
       "0  88305            bonding\n",
       "1  88306          affection\n",
       "2  88307          affection\n",
       "3  88308            bonding\n",
       "4  88309          affection"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_final.to_csv('ans_final.csv', sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
